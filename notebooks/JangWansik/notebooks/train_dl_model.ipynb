{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4db6b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터를 불러와 학습 준비를 시작합니다...\n",
      "딥러닝 모델 학습을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_24816\\1785872245.py:129: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(model, dummy_input, onnx_output, input_names=['input'], output_names=['output'], dynamic_axes={'input': {0: 'batch_size'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ChurnDNN([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ChurnDNN([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "학습 완료! [정확도: 0.7980, 리콜: 0.8962, F1: 0.7383]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "# 노트북이 notebooks 폴더에 있으므로 상위 폴더 경로를 사용합니다\n",
    "data_path = '../data/spotify_churn_dataset.csv'\n",
    "onnx_output = '../models/spotify_dl_model.onnx'\n",
    "preprocessor_output = '../models/dl_preprocessor.pkl'\n",
    "metrics_json = '../data/model_metrics.json'\n",
    "\n",
    "print(\"데이터를 불러와 학습 준비를 시작합니다...\")\n",
    "df = pd.read_csv(data_path)\n",
    "if 'user_id' in df.columns:\n",
    "    df = df.drop(columns=['user_id'])\n",
    "\n",
    "# 의미 있는 분석을 위한 파생 변수들을 생성합니다\n",
    "df['ad_burden'] = df['ads_listened_per_week'] / (df['listening_time'] + 1)\n",
    "df['satisfaction_score'] = df['songs_played_per_day'] * (1 - df['skip_rate'])\n",
    "df['time_per_song'] = df['listening_time'] / (df['songs_played_per_day'] + 1)\n",
    "\n",
    "X = df.drop(columns=['is_churned'])\n",
    "y = df['is_churned']\n",
    "\n",
    "# 데이터를 학습, 검증, 테스트용으로 6:2:2 비율로 나눕니다\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# 수치형과 범주형 데이터를 각각 처리하는 파이프라인입니다\n",
    "num_features = ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'ads_listened_per_week', 'offline_listening', 'ad_burden', 'satisfaction_score', 'time_per_song']\n",
    "cat_features = ['gender', 'country', 'subscription_type', 'device_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_features)\n",
    "    ])\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# 데이터 불균형을 해결하기 위해 학습 데이터에만 SMOTE를 적용합니다\n",
    "smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_proc, y_train)\n",
    "\n",
    "# 파이토치 모델 학습을 위해 데이터를 텐서로 변환합니다\n",
    "X_train_t = torch.FloatTensor(X_train_res)\n",
    "y_train_t = torch.FloatTensor(y_train_res.values).view(-1, 1)\n",
    "X_val_t = torch.FloatTensor(X_val_proc)\n",
    "y_val_t = torch.FloatTensor(y_val.values).view(-1, 1)\n",
    "X_test_t = torch.FloatTensor(X_test_proc)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\n",
    "\n",
    "# 추천드린 LayerNorm을 적용한 신경망 구조입니다\n",
    "class ChurnDNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ChurnDNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "model = ChurnDNN(input_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 학습을 진행하며 가장 성능이 좋은 시점의 가중치를 저장합니다\n",
    "print(\"딥러닝 모델 학습을 시작합니다...\")\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(150):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss_fn(model(batch_x), batch_y).backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_l = loss_fn(model(X_val_t), y_val_t)\n",
    "        if val_l < best_val_loss:\n",
    "            best_val_loss = val_l\n",
    "            torch.save(model.state_dict(), 'best_temp.pt')\n",
    "\n",
    "# 저장된 베스트 가중치를 불러와 ONNX 파일로 변환합니다\n",
    "model.load_state_dict(torch.load('best_temp.pt'))\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, input_dim)\n",
    "torch.onnx.export(model, dummy_input, onnx_output, input_names=['input'], output_names=['output'], dynamic_axes={'input': {0: 'batch_size'}})\n",
    "os.remove('best_temp.pt')\n",
    "\n",
    "# 최적의 임계값을 찾아 최종 성능을 측정합니다\n",
    "with torch.no_grad():\n",
    "    y_scores = model(X_test_t).numpy().flatten()\n",
    "\n",
    "best_threshold, top_f1 = 0.5, 0\n",
    "for t in np.arange(0.3, 0.7, 0.01):\n",
    "    current_f1 = f1_score(y_test, (y_scores >= t).astype(int))\n",
    "    if current_f1 > top_f1:\n",
    "        top_f1 = current_f1\n",
    "        best_threshold = t\n",
    "\n",
    "# 최종 지표 계산 및 결과 저장\n",
    "y_final_pred = (y_scores >= best_threshold).astype(int)\n",
    "final_acc = accuracy_score(y_test, y_final_pred)\n",
    "final_rec = recall_score(y_test, y_final_pred)\n",
    "\n",
    "try:\n",
    "    with open(metrics_json, 'r') as f:\n",
    "        all_metrics = json.load(f)\n",
    "except:\n",
    "    all_metrics = {}\n",
    "\n",
    "all_metrics['Deep Learning (DNN)'] = {\n",
    "    'Accuracy': float(final_acc),\n",
    "    'Recall': float(final_rec),\n",
    "    'F1-Score': float(top_f1),\n",
    "    'Best Threshold': float(best_threshold)\n",
    "}\n",
    "\n",
    "with open(metrics_json, 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=4)\n",
    "\n",
    "joblib.dump(preprocessor, preprocessor_output)\n",
    "print(f\"학습 완료! [정확도: {final_acc:.4f}, 리콜: {final_rec:.4f}, F1: {top_f1:.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
